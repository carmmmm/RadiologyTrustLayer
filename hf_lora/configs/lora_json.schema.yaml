# LoRA config: JSON schema compliance adapter
model_id: google/medgemma-4b-it
task_type: CAUSAL_LM

lora:
  r: 16
  lora_alpha: 32
  target_modules:
    - q_proj
    - v_proj
    - k_proj
    - o_proj
  lora_dropout: 0.05
  bias: none

training:
  num_train_epochs: 3
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 4
  learning_rate: 2.0e-4
  warmup_ratio: 0.03
  lr_scheduler_type: cosine
  fp16: false
  bf16: true
  max_seq_length: 2048
  logging_steps: 10
  save_steps: 100
  eval_steps: 100
  save_total_limit: 2

data:
  train_file: hf_lora/dataset/train.jsonl
  eval_file: hf_lora/dataset/eval.jsonl
  objective: json_schema_compliance

output:
  output_dir: hf_lora/checkpoints/json_schema
  hub_repo: rtl-medgemma-lora-json
