{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTL LoRA Adapter Training\n",
    "\n",
    "**Radiology Trust Layer — MedGemma Impact Challenge**\n",
    "\n",
    "This notebook fine-tunes [MedGemma-4B-IT](https://huggingface.co/google/medgemma-4b-it) with a LoRA adapter to improve:\n",
    "1. **JSON schema compliance** — structured output reliability for the RTL audit pipeline\n",
    "2. **Uncertainty calibration** — reduced overconfident language in alignment labels\n",
    "\n",
    "The trained adapter is published to Hugging Face Hub for use in the [RTL live demo](https://huggingface.co/spaces/outlawpink/RadiologyTrustLayer).\n",
    "\n",
    "**Requirements:**\n",
    "- Kaggle GPU T4 accelerator\n",
    "- `HF_TOKEN` added as a Kaggle secret\n",
    "- MedGemma license accepted at https://huggingface.co/google/medgemma-4b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q peft trl transformers accelerate datasets bitsandbytes huggingface_hub sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with Hugging Face Hub\n",
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "secrets = UserSecretsClient()\n",
    "HF_TOKEN = secrets.get_secret(\"HF_TOKEN\")\n",
    "login(token=HF_TOKEN)\n",
    "print(\"Logged in to Hugging Face Hub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Generate synthetic training data\n",
    "# Adapted from: hf_lora/dataset/make_synthetic.py\n",
    "# ============================================================\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "OUTPUT_DIR = Path(\"/kaggle/working\")\n",
    "\n",
    "CLAIM_TEMPLATES = [\n",
    "    (\"There is {finding} in the {location}.\", \"finding\"),\n",
    "    (\"No {finding} is identified.\", \"absence\"),\n",
    "    (\"The {finding} measures {size} cm.\", \"measurement\"),\n",
    "    (\"Findings are consistent with {diagnosis}.\", \"impression\"),\n",
    "    (\"{finding} is noted, possibly representing {diagnosis}.\", \"impression\"),\n",
    "    (\"Mild {finding} is present.\", \"finding\"),\n",
    "    (\"The {location} appears within normal limits.\", \"finding\"),\n",
    "]\n",
    "\n",
    "FINDINGS = [\"consolidation\", \"opacity\", \"effusion\", \"atelectasis\", \"pneumothorax\",\n",
    "            \"infiltrate\", \"nodule\", \"mass\", \"cardiomegaly\", \"hyperinflation\"]\n",
    "LOCATIONS = [\"right lower lobe\", \"left upper lobe\", \"bilateral lung bases\",\n",
    "             \"right hemithorax\", \"left costophrenic angle\", \"mediastinum\", \"right hilum\"]\n",
    "DIAGNOSES = [\"pneumonia\", \"heart failure\", \"COPD\", \"pulmonary edema\", \"lung cancer\",\n",
    "             \"pleural effusion\", \"atelectasis\"]\n",
    "SIZES = [str(round(random.uniform(0.5, 4.0), 1)) for _ in range(20)]\n",
    "LABELS = [\"supported\", \"uncertain\", \"not_assessable\", \"needs_review\"]\n",
    "\n",
    "OVERCONFIDENT_PHRASES = [\n",
    "    (\"There is definitely\", \"There appears to be\"),\n",
    "    (\"This is consistent with\", \"Findings may be consistent with\"),\n",
    "    (\"Clearly shows\", \"Suggests\"),\n",
    "    (\"No doubt\", \"Possibly\"),\n",
    "    (\"Confirms\", \"May suggest\"),\n",
    "]\n",
    "\n",
    "\n",
    "def make_claim(i):\n",
    "    template, ctype = random.choice(CLAIM_TEMPLATES)\n",
    "    text = template.format(\n",
    "        finding=random.choice(FINDINGS),\n",
    "        location=random.choice(LOCATIONS),\n",
    "        size=random.choice(SIZES),\n",
    "        diagnosis=random.choice(DIAGNOSES),\n",
    "    )\n",
    "    return {\"claim_id\": f\"c{i+1}\", \"text\": text,\n",
    "            \"sentence_span\": {\"start\": i*60, \"end\": i*60+len(text)}, \"claim_type\": ctype}\n",
    "\n",
    "\n",
    "def make_alignment_example(n_claims=4):\n",
    "    claims = [make_claim(i) for i in range(n_claims)]\n",
    "    alignments = []\n",
    "    for claim in claims:\n",
    "        label = random.choices(LABELS, weights=[0.5, 0.25, 0.15, 0.1])[0]\n",
    "        alignments.append({\n",
    "            \"claim_id\": claim[\"claim_id\"],\n",
    "            \"label\": label,\n",
    "            \"evidence\": f\"Visual evidence {'supports' if label=='supported' else 'does not clearly support'} this claim.\",\n",
    "            \"confidence\": round(random.uniform(0.5, 0.95), 2),\n",
    "            \"related_finding_ids\": [f\"f{random.randint(1,3)}\"],\n",
    "            \"claim_text\": claim[\"text\"],\n",
    "        })\n",
    "    return {\"claims\": claims, \"alignments\": alignments}\n",
    "\n",
    "\n",
    "def make_json_compliance_pair():\n",
    "    example = make_alignment_example()\n",
    "    claims_json = json.dumps(example[\"claims\"], indent=2)\n",
    "    schema = json.dumps({\"type\": \"object\", \"required\": [\"alignments\"],\n",
    "                         \"properties\": {\"alignments\": {\"type\": \"array\"}}})\n",
    "    prompt = f\"Align the following claims to image findings.\\nClaims:\\n{claims_json}\\nRespond with JSON matching: {schema}\"\n",
    "    good = json.dumps({\"alignments\": example[\"alignments\"]}, indent=2)\n",
    "    return {\"prompt\": prompt, \"good\": good}\n",
    "\n",
    "\n",
    "def make_uncertainty_pair():\n",
    "    claim_text = make_claim(0)[\"text\"]\n",
    "    calibrated = claim_text\n",
    "    original = random.choice([\"Definite\", \"Clearly\", \"Obviously\", \"\"]) + \" \" + claim_text\n",
    "    original = original.strip()\n",
    "    for over, cal in OVERCONFIDENT_PHRASES:\n",
    "        if over.lower() in original.lower():\n",
    "            calibrated = original.replace(over, cal)\n",
    "            break\n",
    "    return {\"overconfident\": original, \"calibrated\": calibrated}\n",
    "\n",
    "\n",
    "def generate_dataset(n_train=200, n_eval=50):\n",
    "    # JSON schema compliance\n",
    "    with open(OUTPUT_DIR / \"train.jsonl\", \"w\") as f:\n",
    "        for _ in range(n_train):\n",
    "            pair = make_json_compliance_pair()\n",
    "            f.write(json.dumps({\"prompt\": pair[\"prompt\"], \"completion\": pair[\"good\"]}) + \"\\n\")\n",
    "    with open(OUTPUT_DIR / \"eval.jsonl\", \"w\") as f:\n",
    "        for _ in range(n_eval):\n",
    "            pair = make_json_compliance_pair()\n",
    "            f.write(json.dumps({\"prompt\": pair[\"prompt\"], \"completion\": pair[\"good\"]}) + \"\\n\")\n",
    "\n",
    "    # Uncertainty calibration\n",
    "    with open(OUTPUT_DIR / \"uncertainty_train.jsonl\", \"w\") as f:\n",
    "        for _ in range(n_train):\n",
    "            pair = make_uncertainty_pair()\n",
    "            f.write(json.dumps({\"input\": pair[\"overconfident\"], \"output\": pair[\"calibrated\"]}) + \"\\n\")\n",
    "    with open(OUTPUT_DIR / \"uncertainty_eval.jsonl\", \"w\") as f:\n",
    "        for _ in range(n_eval):\n",
    "            pair = make_uncertainty_pair()\n",
    "            f.write(json.dumps({\"input\": pair[\"overconfident\"], \"output\": pair[\"calibrated\"]}) + \"\\n\")\n",
    "\n",
    "    print(f\"Generated {n_train} train + {n_eval} eval pairs\")\n",
    "\n",
    "\n",
    "generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Format data for Gemma chat template\n",
    "# Adapted from: hf_lora/dataset/format.py\n",
    "# ============================================================\n",
    "\n",
    "def format_for_chat(input_path, output_path):\n",
    "    \"\"\"Convert prompt/completion pairs to Gemma chat template.\"\"\"\n",
    "    count = 0\n",
    "    with open(input_path) as fin, open(output_path, \"w\") as fout:\n",
    "        for line in fin:\n",
    "            pair = json.loads(line.strip())\n",
    "            prompt = pair.get(\"prompt\", pair.get(\"input\", \"\"))\n",
    "            completion = pair.get(\"completion\", pair.get(\"output\", \"\"))\n",
    "            text = (\n",
    "                f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n\"\n",
    "                f\"<start_of_turn>model\\n{completion}<end_of_turn>\"\n",
    "            )\n",
    "            fout.write(json.dumps({\"text\": text}) + \"\\n\")\n",
    "            count += 1\n",
    "    print(f\"Formatted {count} pairs -> {output_path}\")\n",
    "\n",
    "\n",
    "# Format all datasets\n",
    "for split in [\"train\", \"eval\"]:\n",
    "    format_for_chat(OUTPUT_DIR / f\"{split}.jsonl\", OUTPUT_DIR / f\"{split}_chat.jsonl\")\n",
    "for split in [\"train\", \"eval\"]:\n",
    "    format_for_chat(OUTPUT_DIR / f\"uncertainty_{split}.jsonl\", OUTPUT_DIR / f\"uncertainty_{split}_chat.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load MedGemma with 8-bit quantization (fits in T4 16GB)\n",
    "# ============================================================\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, BitsAndBytesConfig\n",
    "\n",
    "MODEL_ID = \"google/medgemma-4b-it\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "print(f\"Loading {MODEL_ID} with 8-bit quantization...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "print(f\"Model loaded. Device: {model.device}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Configure and apply LoRA adapter\n",
    "# ============================================================\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Train with SFTTrainer\n",
    "# ============================================================\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "CHECKPOINT_DIR = \"/kaggle/working/rtl-lora-checkpoint\"\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files={\n",
    "    \"train\": str(OUTPUT_DIR / \"train_chat.jsonl\"),\n",
    "    \"validation\": str(OUTPUT_DIR / \"eval_chat.jsonl\"),\n",
    "})\n",
    "\n",
    "print(f\"Train: {len(dataset['train'])} samples, Eval: {len(dataset['validation'])} samples\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CHECKPOINT_DIR,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    bf16=False,\n",
    "    fp16=True,\n",
    "    max_grad_norm=1.0,\n",
    "    logging_steps=10,\n",
    "    save_steps=50,\n",
    "    eval_steps=50,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=1024,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "trainer.save_model(CHECKPOINT_DIR)\n",
    "print(f\"Training complete. Model saved to {CHECKPOINT_DIR}\")\n",
    "\n",
    "# Save training summary\n",
    "import json\n",
    "summary = {\n",
    "    \"model_id\": MODEL_ID,\n",
    "    \"lora_r\": 8,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"target_modules\": [\"q_proj\", \"v_proj\"],\n",
    "    \"train_samples\": len(dataset[\"train\"]),\n",
    "    \"eval_samples\": len(dataset[\"validation\"]),\n",
    "    \"epochs\": 3,\n",
    "    \"quantization\": \"8-bit\",\n",
    "}\n",
    "with open(f\"{CHECKPOINT_DIR}/training_summary.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(\"Training summary saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Evaluate before/after LoRA\n",
    "# Adapted from: hf_lora/eval/eval_lora_before_after.py\n",
    "# ============================================================\n",
    "import re\n",
    "\n",
    "OVERCONFIDENT_PATTERNS = [\n",
    "    r\"\\bdefinitely\\b\", r\"\\bclearly\\b\", r\"\\bobviously\\b\", r\"\\bconfirms\\b\",\n",
    "    r\"\\bno doubt\\b\", r\"\\bwithout question\\b\", r\"\\bconclusively\\b\",\n",
    "]\n",
    "\n",
    "\n",
    "def is_json_valid(text):\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "        return \"alignments\" in data and isinstance(data[\"alignments\"], list)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def try_extract_json(text):\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        m = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
    "        if m:\n",
    "            try:\n",
    "                return json.loads(m.group(0))\n",
    "            except Exception:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def has_overconfident_language(text):\n",
    "    return any(re.search(p, text.lower()) for p in OVERCONFIDENT_PATTERNS)\n",
    "\n",
    "\n",
    "def evaluate_model(model_fn, test_cases):\n",
    "    n = len(test_cases)\n",
    "    json_valid = 0\n",
    "    overconfident = 0\n",
    "    schema_repaired = 0\n",
    "\n",
    "    for case in test_cases:\n",
    "        prompt = case[\"prompt\"]\n",
    "        try:\n",
    "            output = model_fn(prompt)\n",
    "        except Exception:\n",
    "            output = \"\"\n",
    "\n",
    "        if is_json_valid(output):\n",
    "            json_valid += 1\n",
    "        else:\n",
    "            parsed = try_extract_json(output)\n",
    "            if parsed and \"alignments\" in parsed:\n",
    "                json_valid += 1\n",
    "                schema_repaired += 1\n",
    "\n",
    "        if has_overconfident_language(output):\n",
    "            overconfident += 1\n",
    "\n",
    "    return {\n",
    "        \"n\": n,\n",
    "        \"json_valid_rate\": json_valid / n,\n",
    "        \"overconfidence_rate\": overconfident / n,\n",
    "        \"schema_repair_rate\": schema_repaired / n,\n",
    "    }\n",
    "\n",
    "\n",
    "# Load test cases\n",
    "test_cases = []\n",
    "with open(OUTPUT_DIR / \"eval.jsonl\") as f:\n",
    "    for line in f:\n",
    "        test_cases.append(json.loads(line.strip()))\n",
    "test_cases = test_cases[:50]\n",
    "\n",
    "\n",
    "# Create inference function\n",
    "def make_inference_fn(m):\n",
    "    def fn(prompt):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(m.device)\n",
    "        with torch.no_grad():\n",
    "            out = m.generate(**inputs, max_new_tokens=512, do_sample=False)\n",
    "        return tokenizer.decode(out[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    return fn\n",
    "\n",
    "\n",
    "# Evaluate LoRA model\n",
    "print(\"Evaluating LoRA model on\", len(test_cases), \"cases...\")\n",
    "lora_fn = make_inference_fn(model)\n",
    "lora_metrics = evaluate_model(lora_fn, test_cases)\n",
    "\n",
    "print(\"\\n=== LoRA Model Results ===\")\n",
    "print(f\"JSON Valid Rate:      {lora_metrics['json_valid_rate']:.1%}\")\n",
    "print(f\"Overconfidence Rate:  {lora_metrics['overconfidence_rate']:.1%}\")\n",
    "print(f\"Schema Repair Rate:   {lora_metrics['schema_repair_rate']:.1%}\")\n",
    "\n",
    "# Save metrics\n",
    "with open(f\"{CHECKPOINT_DIR}/eval_metrics.json\", \"w\") as f:\n",
    "    json.dump({\"lora_model\": lora_metrics, \"test_set_size\": len(test_cases)}, f, indent=2)\n",
    "print(f\"\\nMetrics saved to {CHECKPOINT_DIR}/eval_metrics.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Push adapter to Hugging Face Hub\n",
    "# ============================================================\n",
    "from huggingface_hub import HfApi, upload_folder\n",
    "\n",
    "REPO_ID = \"outlawpink/rtl-medgemma-lora\"\n",
    "\n",
    "api = HfApi()\n",
    "api.create_repo(repo_id=REPO_ID, exist_ok=True, repo_type=\"model\")\n",
    "\n",
    "# Generate model card\n",
    "metrics = lora_metrics\n",
    "model_card = f\"\"\"---\n",
    "library_name: peft\n",
    "base_model: google/medgemma-4b-it\n",
    "tags:\n",
    "  - medical\n",
    "  - radiology\n",
    "  - lora\n",
    "  - peft\n",
    "  - medgemma\n",
    "  - rtl\n",
    "license: apache-2.0\n",
    "---\n",
    "\n",
    "# RTL LoRA Adapter for MedGemma\n",
    "\n",
    "LoRA adapter for `google/medgemma-4b-it` trained for the **Radiology Trust Layer (RTL)** project.\n",
    "\n",
    "## What this adapter does\n",
    "\n",
    "- Improves **JSON schema compliance** in structured output tasks\n",
    "- Reduces **overconfident language** in uncertainty alignment tasks\n",
    "- Trained on synthetic radiology QA pairs (no PHI)\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\"google/medgemma-4b-it\")\n",
    "model = PeftModel.from_pretrained(base, \"{REPO_ID}\")\n",
    "model = model.merge_and_unload()\n",
    "```\n",
    "\n",
    "## Training Details\n",
    "\n",
    "| Setting | Value |\n",
    "|---------|-------|\n",
    "| Base model | `google/medgemma-4b-it` |\n",
    "| LoRA rank | 8 |\n",
    "| LoRA alpha | 16 |\n",
    "| Target modules | q_proj, v_proj |\n",
    "| Training samples | 200 |\n",
    "| Quantization | 8-bit (bitsandbytes) |\n",
    "| Precision | fp16 |\n",
    "| Framework | PEFT + TRL SFTTrainer |\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "| Metric | Score |\n",
    "|--------|-------|\n",
    "| JSON Valid Rate | {metrics['json_valid_rate']:.1%} |\n",
    "| Overconfidence Rate | {metrics['overconfidence_rate']:.1%} |\n",
    "| Schema Repair Rate | {metrics['schema_repair_rate']:.1%} |\n",
    "\n",
    "## Links\n",
    "\n",
    "- [RTL Live Demo](https://huggingface.co/spaces/outlawpink/RadiologyTrustLayer)\n",
    "- [GitHub Repository](https://github.com/carmmmm/RadiologyTrustLayer)\n",
    "\n",
    "## Disclaimer\n",
    "\n",
    "This adapter is for **research purposes only**. Not intended for clinical use.\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{CHECKPOINT_DIR}/README.md\", \"w\") as f:\n",
    "    f.write(model_card)\n",
    "\n",
    "# Upload\n",
    "url = upload_folder(\n",
    "    folder_path=CHECKPOINT_DIR,\n",
    "    repo_id=REPO_ID,\n",
    "    commit_message=\"Upload RTL LoRA adapter\",\n",
    ")\n",
    "print(f\"\\nAdapter published: https://huggingface.co/{REPO_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "The LoRA adapter has been published to Hugging Face Hub.\n",
    "\n",
    "**To use it in the RTL app:**\n",
    "\n",
    "1. Go to your HF Space settings: https://huggingface.co/spaces/outlawpink/RadiologyTrustLayer/settings\n",
    "2. Add variable: `RTL_LORA_ID=outlawpink/rtl-medgemma-lora`\n",
    "3. Check the \"Use RTL LoRA adapter\" checkbox when running an audit\n",
    "\n",
    "**Published model:** https://huggingface.co/outlawpink/rtl-medgemma-lora"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
